{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Integration Part 1: Data Collection and Preprocessing\n",
    "\n",
    "This notebook implements the first part of the HPC MLOps pipeline, focusing on:\n",
    "1. Setting up the environment and folder structure\n",
    "2. Configuring Kaggle API credentials\n",
    "3. Defining HPC cost tiers in YAML format\n",
    "4. Downloading and processing HPC datasets from Kaggle\n",
    "5. Implementing TomTom O/D Analysis with proper error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas==1.4.0 requests==2.27.1 geopandas==0.10.2 shapely==1.8.1 PyYAML==6.0 kaggle==1.5.12 numpy==1.21.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import kaggle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display pandas dataframes with more columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary folders\n",
    "folders = ['data', 'config', 'notebooks', 'scripts']\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"Created folder: {folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Kaggle API Credentials\n",
    "\n",
    "Note: You'll need to upload your kaggle.json file or create it with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set up Kaggle credentials\n",
    "def setup_kaggle_credentials():\n",
    "    \"\"\"\n",
    "    Set up Kaggle API credentials.\n",
    "    You can either upload your kaggle.json file or create it with your credentials.\n",
    "    \"\"\"\n",
    "    # Create .kaggle directory if it doesn't exist\n",
    "    os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "    \n",
    "    # Check if kaggle.json exists\n",
    "    kaggle_json_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    \n",
    "    if not os.path.exists(kaggle_json_path):\n",
    "        print(\"Kaggle credentials not found. Please enter your Kaggle username and API key:\")\n",
    "        username = input(\"Kaggle username: \")\n",
    "        key = input(\"Kaggle API key: \")\n",
    "        \n",
    "        # Create kaggle.json file\n",
    "        with open(kaggle_json_path, 'w') as f:\n",
    "            json.dump({\"username\": username, \"key\": key}, f)\n",
    "        \n",
    "        # Set permissions\n",
    "        os.chmod(kaggle_json_path, 0o600)\n",
    "        \n",
    "        print(\"Kaggle credentials saved successfully.\")\n",
    "    else:\n",
    "        print(\"Kaggle credentials found.\")\n",
    "\n",
    "# Set up Kaggle credentials\n",
    "setup_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define HPC Cost Tiers in YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HPC cost tiers\n",
    "hpc_cost_tiers = {\n",
    "    'tiers': {\n",
    "        '350kW': {\n",
    "            'power_output': 350,\n",
    "            'installation_cost': 150000,\n",
    "            'maintenance_annual': 15000,\n",
    "            'electricity_cost_per_kwh': 0.15,\n",
    "            'charging_fee_per_kwh': 0.45,\n",
    "            'charging_fee_per_minute': 0.40,\n",
    "            'average_session_duration_minutes': 25,\n",
    "            'average_energy_per_session_kwh': 35\n",
    "        },\n",
    "        '1000kW': {\n",
    "            'power_output': 1000,\n",
    "            'installation_cost': 350000,\n",
    "            'maintenance_annual': 35000,\n",
    "            'electricity_cost_per_kwh': 0.12,\n",
    "            'charging_fee_per_kwh': 0.50,\n",
    "            'charging_fee_per_minute': 0.60,\n",
    "            'average_session_duration_minutes': 15,\n",
    "            'average_energy_per_session_kwh': 40\n",
    "        }\n",
    "    },\n",
    "    'global_parameters': {\n",
    "        'land_lease_annual': 50000,\n",
    "        'staff_cost_annual': 120000,\n",
    "        'expected_sessions_per_day': 25,\n",
    "        'expected_utilization_rate': 0.65,\n",
    "        'expected_growth_rate_annual': 0.15\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to YAML file\n",
    "with open('config/hpc_cost_tiers.yaml', 'w') as f:\n",
    "    yaml.dump(hpc_cost_tiers, f, default_flow_style=False)\n",
    "\n",
    "print(\"HPC cost tiers saved to config/hpc_cost_tiers.yaml\")\n",
    "\n",
    "# Display the YAML content\n",
    "with open('config/hpc_cost_tiers.yaml', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download and Process HPC Datasets from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and process Kaggle datasets\n",
    "def download_kaggle_dataset(dataset_name, file_pattern, output_file, sample_size=None):\n",
    "    \"\"\"\n",
    "    Download a dataset from Kaggle and optionally sample it.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the Kaggle dataset\n",
    "        file_pattern: Pattern to match the CSV file in the dataset\n",
    "        output_file: Path to save the processed dataset\n",
    "        sample_size: Number of rows to sample (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with the processed dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download the dataset\n",
    "        print(f\"Downloading {dataset_name}...\")\n",
    "        kaggle.api.dataset_download_files(dataset_name, path='data', unzip=True)\n",
    "        \n",
    "        # Find the CSV file\n",
    "        csv_files = [f for f in os.listdir('data') if f.endswith('.csv') and file_pattern in f]\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No CSV file matching '{file_pattern}' found in the dataset.\")\n",
    "            return None\n",
    "        \n",
    "        csv_file = os.path.join('data', csv_files[0])\n",
    "        print(f\"Found CSV file: {csv_file}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"Original dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Sample the dataset if requested\n",
    "        if sample_size is not None and sample_size < len(df):\n",
    "            df = df.sample(sample_size, random_state=42)\n",
    "            print(f\"Sampled dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Save the processed dataset\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed dataset saved to {output_file}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        \n",
    "        # Create synthetic data as fallback\n",
    "        print(\"Creating synthetic data as fallback...\")\n",
    "        df = create_synthetic_ev_data(sample_size or 1000)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Synthetic dataset saved to {output_file}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Function to create synthetic EV charging data\n",
    "def create_synthetic_ev_data(n_rows=1000):\n",
    "    \"\"\"\n",
    "    Create synthetic EV charging data.\n",
    "    \n",
    "    Args:\n",
    "        n_rows: Number of rows to generate\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with synthetic data\n",
    "    \"\"\"\n",
    "    # Define possible values\n",
    "    station_ids = [f\"station_{i}\" for i in range(1, 51)]\n",
    "    vehicle_types = ['Tesla Model 3', 'Tesla Model Y', 'Nissan Leaf', 'Chevy Bolt', 'Ford Mustang Mach-E', \n",
    "                     'Hyundai Kona Electric', 'Kia EV6', 'Volkswagen ID.4', 'Audi e-tron', 'Rivian R1T']\n",
    "    connector_types = ['CCS', 'CHAdeMO', 'Tesla Supercharger', 'Type 2']\n",
    "    cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', \n",
    "              'San Antonio', 'San Diego', 'Dallas', 'San Francisco']\n",
    "    \n",
    "    # Generate random data\n",
    "    data = {\n",
    "        'session_id': [f\"session_{i}\" for i in range(1, n_rows+1)],\n",
    "        'station_id': [random.choice(station_ids) for _ in range(n_rows)],\n",
    "        'vehicle_type': [random.choice(vehicle_types) for _ in range(n_rows)],\n",
    "        'connector_type': [random.choice(connector_types) for _ in range(n_rows)],\n",
    "        'start_time': [datetime.now().strftime('%Y-%m-%d %H:%M:%S') for _ in range(n_rows)],\n",
    "        'duration_minutes': [random.randint(15, 120) for _ in range(n_rows)],\n",
    "        'energy_kwh': [random.uniform(10, 80) for _ in range(n_rows)],\n",
    "        'city': [random.choice(cities) for _ in range(n_rows)],\n",
    "        'latitude': [random.uniform(25, 48) for _ in range(n_rows)],\n",
    "        'longitude': [random.uniform(-125, -70) for _ in range(n_rows)],\n",
    "        'temperature_c': [random.uniform(-10, 35) for _ in range(n_rows)],\n",
    "        'is_weekday': [random.choice([True, False]) for _ in range(n_rows)],\n",
    "        'hour_of_day': [random.randint(0, 23) for _ in range(n_rows)]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process the first dataset\n",
    "dataset1 = download_kaggle_dataset(\n",
    "    dataset_name='nicholassjtu/ev-charging-station-data',\n",
    "    file_pattern='charging',\n",
    "    output_file='data/ev_charging_patterns.csv',\n",
    "    sample_size=5000\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "if dataset1 is not None:\n",
    "    print(\"\\nFirst few rows of the first dataset:\")\n",
    "    display(dataset1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process the second dataset\n",
    "dataset2 = download_kaggle_dataset(\n",
    "    dataset_name='anmolkumar/electric-vehicle-charging-dataset',\n",
    "    file_pattern='electric',\n",
    "    output_file='data/ev_charging_sessions.csv',\n",
    "    sample_size=5000\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "if dataset2 is not None:\n",
    "    print(\"\\nFirst few rows of the second dataset:\")\n",
    "    display(dataset2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process the third dataset\n",
    "dataset3 = download_kaggle_dataset(\n",
    "    dataset_name='ipingou/ev-charging-stations',\n",
    "    file_pattern='station',\n",
    "    output_file='data/ev_charging_stations.csv',\n",
    "    sample_size=1000\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "if dataset3 is not None:\n",
    "    print(\"\\nFirst few rows of the third dataset:\")\n",
    "    display(dataset3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement TomTom O/D Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform TomTom Origin/Destination Analysis\n",
    "def perform_tomtom_od_analysis(api_key, origin_lat, origin_lon, destination_lat, destination_lon):\n",
    "    \"\"\"\n",
    "    Perform Origin/Destination analysis using TomTom API.\n",
    "    \n",
    "    Args:\n",
    "        api_key: TomTom API key\n",
    "        origin_lat: Origin latitude\n",
    "        origin_lon: Origin longitude\n",
    "        destination_lat: Destination latitude\n",
    "        destination_lon: Destination longitude\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with route information or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TomTom Routing API endpoint\n",
    "        url = f\"https://api.tomtom.com/routing/1/calculateRoute/{origin_lat},{origin_lon}:{destination_lat},{destination_lon}/json\"\n",
    "        \n",
    "        # Parameters\n",
    "        params = {\n",
    "            'key': api_key,\n",
    "            'traffic': 'true',\n",
    "            'travelMode': 'car'\n",
    "        }\n",
    "        \n",
    "        # Make the request\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract relevant information\n",
    "            route = data['routes'][0]\n",
    "            summary = route['summary']\n",
    "            \n",
    "            result = {\n",
    "                'distance_meters': summary['lengthInMeters'],\n",
    "                'travel_time_seconds': summary['travelTimeInSeconds'],\n",
    "                'traffic_delay_seconds': summary.get('trafficDelayInSeconds', 0),\n",
    "                'departure_time': datetime.now().isoformat(),\n",
    "                'origin': f\"{origin_lat},{origin_lon}\",\n",
    "                'destination': f\"{destination_lat},{destination_lon}\"\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(f\"TomTom API request failed with status code: {response.status_code}\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error performing TomTom O/D analysis: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TomTom O/D Analysis with error handling\n",
    "def test_tomtom_od_analysis():\n",
    "    \"\"\"\n",
    "    Test TomTom O/D Analysis with proper error handling.\n",
    "    \"\"\"\n",
    "    # TomTom API key (replace with your actual API key)\n",
    "    api_key = \"YOUR_TOMTOM_API_KEY\"\n",
    "    \n",
    "    # Test coordinates (New York to Boston)\n",
    "    origin_lat, origin_lon = 40.7128, -74.0060\n",
    "    destination_lat, destination_lon = 42.3601, -71.0589\n",
    "    \n",
    "    print(\"Testing TomTom O/D Analysis...\")\n",
    "    \n",
    "    # Perform the analysis\n",
    "    result = perform_tomtom_od_analysis(api_key, origin_lat, origin_lon, destination_lat, destination_lon)\n",
    "    \n",
 <response clipped><NOTE>To save on context only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>