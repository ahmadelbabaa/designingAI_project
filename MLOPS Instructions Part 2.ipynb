{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Integration Part 2: Data Integration and ROI Analysis\n",
    "\n",
    "This notebook implements the second part of the HPC MLOps pipeline, focusing on:\n",
    "1. Verifying prerequisites from Part 1\n",
    "2. Loading HPC cost references from YAML\n",
    "3. Merging the three HPC usage datasets with standardized columns\n",
    "4. Implementing usage modeling and forecasting\n",
    "5. Creating ROI calculations for different power tiers\n",
    "6. Handling competitor HPC synergy analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas==1.4.0 numpy==1.21.6 PyYAML==6.0 matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display pandas dataframes with more columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Prerequisites from Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify prerequisites\n",
    "def verify_prerequisites():\n",
    "    \"\"\"\n",
    "    Verify that all prerequisites from Part 1 are available.\n",
    "    If not, create the necessary files with synthetic data.\n",
    "    \"\"\"\n",
    "    # Required files\n",
    "    required_files = {\n",
    "        'config/hpc_cost_tiers.yaml': create_hpc_cost_tiers,\n",
    "        'data/ev_charging_patterns.csv': create_synthetic_ev_patterns,\n",
    "        'data/ev_charging_sessions.csv': create_synthetic_ev_sessions,\n",
    "        'data/ev_charging_stations.csv': create_synthetic_ev_stations\n",
    "    }\n",
    "    \n",
    "    # Create folders if they don't exist\n",
    "    os.makedirs('config', exist_ok=True)\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # Check each required file\n",
    "    all_exist = True\n",
    "    for file_path, create_func in required_files.items():\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"✅ {file_path} exists\")\n",
    "        else:\n",
    "            print(f\"❌ {file_path} does not exist, creating synthetic data...\")\n",
    "            create_func()\n",
    "            all_exist = False\n",
    "    \n",
    "    return all_exist\n",
    "\n",
    "# Function to create HPC cost tiers\n",
    "def create_hpc_cost_tiers():\n",
    "    \"\"\"\n",
    "    Create HPC cost tiers YAML file.\n",
    "    \"\"\"\n",
    "    hpc_cost_tiers = {\n",
    "        'tiers': {\n",
    "            '350kW': {\n",
    "                'power_output': 350,\n",
    "                'installation_cost': 150000,\n",
    "                'maintenance_annual': 15000,\n",
    "                'electricity_cost_per_kwh': 0.15,\n",
    "                'charging_fee_per_kwh': 0.45,\n",
    "                'charging_fee_per_minute': 0.40,\n",
    "                'average_session_duration_minutes': 25,\n",
    "                'average_energy_per_session_kwh': 35\n",
    "            },\n",
    "            '1000kW': {\n",
    "                'power_output': 1000,\n",
    "                'installation_cost': 350000,\n",
    "                'maintenance_annual': 35000,\n",
    "                'electricity_cost_per_kwh': 0.12,\n",
    "                'charging_fee_per_kwh': 0.50,\n",
    "                'charging_fee_per_minute': 0.60,\n",
    "                'average_session_duration_minutes': 15,\n",
    "                'average_energy_per_session_kwh': 40\n",
    "            }\n",
    "        },\n",
    "        'global_parameters': {\n",
    "            'land_lease_annual': 50000,\n",
    "            'staff_cost_annual': 120000,\n",
    "            'expected_sessions_per_day': 25,\n",
    "            'expected_utilization_rate': 0.65,\n",
    "            'expected_growth_rate_annual': 0.15\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to YAML file\n",
    "    with open('config/hpc_cost_tiers.yaml', 'w') as f:\n",
    "        yaml.dump(hpc_cost_tiers, f, default_flow_style=False)\n",
    "    \n",
    "    print(\"Created config/hpc_cost_tiers.yaml\")\n",
    "\n",
    "# Function to create synthetic EV charging patterns\n",
    "def create_synthetic_ev_patterns(n_rows=5000):\n",
    "    \"\"\"\n",
    "    Create synthetic EV charging patterns data.\n",
    "    \"\"\"\n",
    "    # Define possible values\n",
    "    station_ids = [f\"station_{i}\" for i in range(1, 51)]\n",
    "    vehicle_types = ['Tesla Model 3', 'Tesla Model Y', 'Nissan Leaf', 'Chevy Bolt', 'Ford Mustang Mach-E', \n",
    "                     'Hyundai Kona Electric', 'Kia EV6', 'Volkswagen ID.4', 'Audi e-tron', 'Rivian R1T']\n",
    "    connector_types = ['CCS', 'CHAdeMO', 'Tesla Supercharger', 'Type 2']\n",
    "    \n",
    "    # Generate random data\n",
    "    data = {\n",
    "        'session_id': [f\"session_{i}\" for i in range(1, n_rows+1)],\n",
    "        'station_id': [np.random.choice(station_ids) for _ in range(n_rows)],\n",
    "        'vehicle_type': [np.random.choice(vehicle_types) for _ in range(n_rows)],\n",
    "        'connector_type': [np.random.choice(connector_types) for _ in range(n_rows)],\n",
    "        'start_time': [(datetime.now() - timedelta(days=np.random.randint(1, 365))).strftime('%Y-%m-%d %H:%M:%S') for _ in range(n_rows)],\n",
    "        'duration_minutes': [np.random.randint(15, 120) for _ in range(n_rows)],\n",
    "        'energy_kwh': [np.random.uniform(10, 80) for _ in range(n_rows)],\n",
    "        'hour_of_day': [np.random.randint(0, 23) for _ in range(n_rows)],\n",
    "        'day_of_week': [np.random.randint(0, 6) for _ in range(n_rows)],\n",
    "        'is_weekend': [np.random.choice([0, 1]) for _ in range(n_rows)],\n",
    "        'temperature_c': [np.random.uniform(-10, 35) for _ in range(n_rows)]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('data/ev_charging_patterns.csv', index=False)\n",
    "    \n",
    "    print(\"Created data/ev_charging_patterns.csv\")\n",
    "\n",
    "# Function to create synthetic EV charging sessions\n",
    "def create_synthetic_ev_sessions(n_rows=5000):\n",
    "    \"\"\"\n",
    "    Create synthetic EV charging sessions data.\n",
    "    \"\"\"\n",
    "    # Define possible values\n",
    "    station_ids = [f\"station_{i}\" for i in range(1, 51)]\n",
    "    user_ids = [f\"user_{i}\" for i in range(1, 1001)]\n",
    "    payment_methods = ['Credit Card', 'App', 'RFID Card', 'Subscription']\n",
    "    \n",
    "    # Generate random data\n",
    "    data = {\n",
    "        'session_id': [f\"session_{i}\" for i in range(1, n_rows+1)],\n",
    "        'user_id': [np.random.choice(user_ids) for _ in range(n_rows)],\n",
    "        'station_id': [np.random.choice(station_ids) for _ in range(n_rows)],\n",
    "        'start_datetime': [(datetime.now() - timedelta(days=np.random.randint(1, 365))).strftime('%Y-%m-%d %H:%M:%S') for _ in range(n_rows)],\n",
    "        'end_datetime': [(datetime.now() - timedelta(days=np.random.randint(0, 364))).strftime('%Y-%m-%d %H:%M:%S') for _ in range(n_rows)],\n",
    "        'energy_delivered_kwh': [np.random.uniform(10, 80) for _ in range(n_rows)],\n",
    "        'charging_cost': [np.random.uniform(5, 40) for _ in range(n_rows)],\n",
    "        'payment_method': [np.random.choice(payment_methods) for _ in range(n_rows)],\n",
    "        'session_completed': [np.random.choice([True, False], p=[0.95, 0.05]) for _ in range(n_rows)]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('data/ev_charging_sessions.csv', index=False)\n",
    "    \n",
    "    print(\"Created data/ev_charging_sessions.csv\")\n",
    "\n",
    "# Function to create synthetic EV charging stations\n",
    "def create_synthetic_ev_stations(n_rows=1000):\n",
    "    \"\"\"\n",
    "    Create synthetic EV charging stations data.\n",
    "    \"\"\"\n",
    "    # Define possible values\n",
    "    operators = ['ChargePoint', 'EVgo', 'Electrify America', 'Tesla', 'Blink', 'Greenlots']\n",
    "    cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', \n",
    "              'San Antonio', 'San Diego', 'Dallas', 'San Francisco']\n",
    "    states = ['NY', 'CA', 'IL', 'TX', 'AZ', 'PA', 'TX', 'CA', 'TX', 'CA']\n",
    "    status = ['Operational', 'Under Maintenance', 'Planned']\n",
    "    \n",
    "    # Generate random data\n",
    "    data = {\n",
    "        'station_id': [f\"station_{i}\" for i in range(1, n_rows+1)],\n",
    "        'operator': [np.random.choice(operators) for _ in range(n_rows)],\n",
    "        'latitude': [np.random.uniform(25, 48) for _ in range(n_rows)],\n",
    "        'longitude': [np.random.uniform(-125, -70) for _ in range(n_rows)],\n",
    "        'city': [cities[i % len(cities)] for i in range(n_rows)],\n",
    "        'state': [states[i % len(states)] for i in range(n_rows)],\n",
    "        'num_ports': [np.random.randint(1, 10) for _ in range(n_rows)],\n",
    "        'power_kw': [np.random.choice([50, 150, 350]) for _ in range(n_rows)],\n",
    "        'status': [np.random.choice(status, p=[0.8, 0.1, 0.1]) for _ in range(n_rows)],\n",
    "        'installation_date': [(datetime.now() - timedelta(days=np.random.randint(1, 1000))).strftime('%Y-%m-%d') for _ in range(n_rows)]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('data/ev_charging_stations.csv', index=False)\n",
    "    \n",
    "    print(\"Created data/ev_charging_stations.csv\")\n",
    "\n",
    "# Verify prerequisites\n",
    "all_prerequisites_exist = verify_prerequisites()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load HPC Cost References from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HPC cost tiers from YAML\n",
    "def load_hpc_cost_tiers():\n",
    "    \"\"\"\n",
    "    Load HPC cost tiers from YAML file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('config/hpc_cost_tiers.yaml', 'r') as f:\n",
    "            cost_tiers = yaml.safe_load(f)\n",
    "        \n",
    "        print(\"Loaded HPC cost tiers:\")\n",
    "        for tier_name, tier_data in cost_tiers['tiers'].items():\n",
    "            print(f\"\\n{tier_name}:\")\n",
    "            for key, value in tier_data.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(\"\\nGlobal parameters:\")\n",
    "        for key, value in cost_tiers['global_parameters'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return cost_tiers\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading HPC cost tiers: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load HPC cost tiers\n",
    "hpc_cost_tiers = load_hpc_cost_tiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge HPC Usage Datasets with Standardized Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and standardize datasets\n",
    "def load_and_standardize_datasets():\n",
    "    \"\"\"\n",
    "    Load and standardize the three HPC usage datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load datasets\n",
    "        patterns_df = pd.read_csv('data/ev_charging_patterns.csv')\n",
    "        sessions_df = pd.read_csv('data/ev_charging_sessions.csv')\n",
    "        stations_df = pd.read_csv('data/ev_charging_stations.csv')\n",
    "        \n",
    "        print(f\"Loaded datasets with shapes:\")\n",
    "        print(f\"  patterns_df: {patterns_df.shape}\")\n",
    "        print(f\"  sessions_df: {sessions_df.shape}\")\n",
    "        print(f\"  stations_df: {stations_df.shape}\")\n",
    "        \n",
    "        # Standardize patterns dataset\n",
    "        patterns_std = patterns_df.copy()\n",
    "        if 'start_time' in patterns_std.columns:\n",
    "            patterns_std['datetime'] = pd.to_datetime(patterns_std['start_time'])\n",
    "        else:\n",
    "            # Create datetime from other columns if available\n",
    "            patterns_std['datetime'] = pd.to_datetime('2023-01-01') + pd.to_timedelta(np.random.randint(0, 365, size=len(patterns_std)), unit='d')\n",
    "        \n",
    "        patterns_std['energy_kwh'] = patterns_std['energy_kwh'] if 'energy_kwh' in patterns_std.columns else np.random.uniform(10, 80, size=len(patterns_std))\n",
    "        patterns_std['duration_minutes'] = patterns_std['duration_minutes'] if 'duration_minutes' in patterns_std.columns else np.random.randint(15, 120, size=len(patterns_std))\n",
    "        patterns_std['source'] = 'patterns'\n",
    "        \n",
    "        # Standardize sessions dataset\n",
    "        sessions_std = sessions_df.copy()\n",
    "        if 'start_datetime' in sessions_std.columns:\n",
    "            sessions_std['datetime'] = pd.to_datetime(sessions_std['start_datetime'])\n",
    "        else:\n",
    "            # Create datetime from other columns if available\n",
    "            sessions_std['datetime'] = pd.to_datetime('2023-01-01') + pd.to_timedelta(np.random.randint(0, 365, size=len(sessions_std)), unit='d')\n",
    "        \n",
    "        sessions_std['energy_kwh'] = sessions_std['energy_delivered_kwh'] if 'energy_delivered_kwh' in sessions_std.columns else np.random.uniform(10, 80, size=len(sessions_std))\n",
    "        \n",
    "        # Calculate duration if start and end times are available\n",
    "        if 'start_datetime' in sessions_std.columns and 'end_datetime' in sessions_std.columns:\n",
    "            sessions_std['start'] = pd.to_datetime(sessions_std['start_datetime'])\n",
    "            sessions_std['end'] = pd.to_datetime(sessions_std['end_datetime'])\n",
    "            sessions_std['duration_minutes'] = (sessions_std['end'] - sessions_std['start']).dt.total_seconds() / 60\n",
    "        else:\n",
    "            sessions_std['duration_minutes'] = np.random.randint(15, 120, size=len(sessions_std))\n",
    "        \n",
    "        sessions_std['source'] = 'sessions'\n",
    "        \n",
    "        # Select common columns for merging\n",
    "        common_columns = ['station_id', 'datetime', 'energy_kwh', 'duration_minutes', 'source']\n",
    "        \n",
    "        patterns_merged = patterns_std[common_columns].copy()\n",
    "        sessions_merged = sessions_std[common_columns].copy()\n",
    "        \n",
    "        # Merge datasets\n",
    "        merged_df = pd.concat([patterns_merged, sessions<response clipped><NOTE>To save on context only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>